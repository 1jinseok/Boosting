{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiplicative Weights and Boosting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVOgID3ft8WO"
      },
      "source": [
        "# Multiplicative Weights and Boosting\n",
        "\n",
        "This notebook is going to take a relatively different approach to introducing boosting. Instead of introducing boosting within the scope of some machine learning application such as decision trees (which was assumedly covered in a prior week), we will first look more broadly at the general algorithmic framework of multiplicative weights, which has wide applications in fields as diverse as game theory and finance. Then, we will go back to boosting through decision trees and random forests. \n",
        "\n",
        "The idea of aggregating information from many sources has great utility in machine learning and is the basis behind ensemble methods. Boosting is one example of an ensemble method which has had great success in real-life applications. Indeed, the fundamental ideas behind these concepts are universal, and you have seen them in previous classes such as EECS16B through Lagrange Interpolation and Orthogonal Matching Pursuit (OMP). First, let's explore the general experts framework and introduce the multiplicative weights algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B9B_GZHt7wW"
      },
      "source": [
        "# Imports here \n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT2rIpvCKNrY"
      },
      "source": [
        "## An Experts Framework\n",
        "In many scenarios, we may have different models or \"experts\" that are telling us how to go about performing some task. The experts may give conflicting pieces of advice. How should we go about using their advice?\n",
        "\n",
        "In the simplest, one-day scenario, without any prior on the experts, we would equally consider the advice from each of the experts... In other words, for a discrete classification problem we simply choose the majority rule.\n",
        "\n",
        "As a warmup, let's consider a classic scenario where we are trying to classify a handwritten digit, 0 through 9. Suppose we have $n$ experts who give us their opinion on a single data point. Write a function that will take the majority vote for this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9ReEa_xOk30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b77eadf-e9d9-44c3-d4be-004dd7805e21"
      },
      "source": [
        "# Function that returns the majority rule based off the \n",
        "# experts advice. Takes in a list containing each \n",
        "# expert's decision and an int representing the number of experts n,\n",
        "# and returns the digit corresponding to the majority.\n",
        "def discreteClassificationMajority(experts, n):\n",
        "  # maxFreq = 0\n",
        "  # maxIndex = experts[0] \n",
        "  # for i in experts: \n",
        "  #     freq = experts.count(i) \n",
        "  #     if freq > maxFreq: \n",
        "  #         maxFreq = freq \n",
        "  #         maxIndex = i\n",
        "  # return i\n",
        "\n",
        "  return max(set(experts), key = experts.count) \n",
        "\n",
        "n = 5\n",
        "experts1 = [0, 2, 2, 5, 2]\n",
        "experts2 = [0, 0, 2, 5, 7]\n",
        "print(discreteClassificationMajority(experts1, n)) # should return 2.\n",
        "print(discreteClassificationMajority(experts2, n)) # should return 0.\n",
        "\n",
        "\n",
        "#PERHAPS CODE UP CONTINUOUS (JUST AN AVERAGE) HERE!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwNVHPS3QlCg"
      },
      "source": [
        "Now, suppose in the discrete case, the experts had more information than just their first choice decision? Specifically, what if the experts rank the preferences? Two possible methods we could do to take advantage of this extra information are: \n",
        "\n",
        "1. Create a separate weight, or a \"score,\" to each category based on the position it is in the expert's list and take the linear combination of the scores with the choices. \n",
        "\n",
        "\n",
        "2. Do instant runoff voting (keep eliminating the last place and redistributing their votes to the next preference until some candidate has a 50% majority). \n",
        "\n",
        "We will omit the code for time's sake due to added complexity from the redistribution of votes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLqii5rtMOhA"
      },
      "source": [
        "Question 1:\n",
        "\n",
        "What is an advantage of method 1? What is a scenario in which method 1. gives us a result that the original majority vote would not?\n",
        "\n",
        "Answer 1:\n",
        "\n",
        "Method 1 gives us more details on the entire classification rather than just the best choice. So, classes other than just the majority class also have impact.\n",
        "\n",
        "Thus, an example of an instance in which this could lead to more accurate results is: suppose that the majority vote is a tie, or near tie, between multiple classes. But then, suppose one of these majority classes has significantly more votes from experts as the second choice or other near-the-top choices. Then, that class will be rewarded in this method and is more likely to be the best class indeed, while the simple majority vote does not have the depth to foresee this scenario. Say, candidate A narrowly beats candidate B for a majority, but candidate B has more voters for second place and third place than candidate A, thus surpassing A.\n",
        "\n",
        "\n",
        "Question 2:\n",
        "\n",
        "What is an advantage of method 2? What is a scenario in which method 2. gives us a result that the original majority vote would not? \n",
        "\n",
        "Answer 2:\n",
        "\n",
        "\n",
        "Advantage 2 gives us more details on what the experts who didn't pick the majority would have chosen for their top choices, given the opportunity to do so. Thus, we can have a more informed choice at each later stage on the remaining candidates all the way until there is only two classes and every expert is voting on which one or the other. Thus, an example of an instance in which this could lead to more accurate results is when: suppose Candidate A narrowly beats Candidate B for the majority initially, but once the votes are redistributed from bottom up, candidate B gets more of those experts' votes and surpasses candidate A.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxbG9hBBOli8"
      },
      "source": [
        "## Multiplicative Weights Algorithm\n",
        "\n",
        "Back to experts scenario. What about if we have multiple days. Based on the outcome from the previous days until now. Some experts may be more skilled (i.e making accurate decisions more frequently) than others. Thus, we want to weigh their advice more comparatively. This is where the multiplicative weights update algorithm comes in.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii3XgONaOk02"
      },
      "source": [
        "# simple example of what the experts tell you to do \n",
        "# Example with day trading or betting on horse races.\n",
        "# code up weighted avg and weight update method (may use higher order functions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exbMtq38TxYX"
      },
      "source": [
        "What if we had one \"perfect expert\" who always predicts correctly. How could we use that to improve our accuracy over time?\n",
        "\n",
        "Answer:\n",
        "\n",
        "You could construct a halving algorithm - each day, if we make a mistake, throw out all that advised you wrongly. Make maximum of log_2(x) mistakes since by majority rule, at most half are incorrect!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TvpNIqtUSrA"
      },
      "source": [
        "In the real world, no one is perfect. So, we usually can't just throw out anyone who makes a single mistake, since pretty soon we'd end up with no one left to give us advice! So instead, we discount the incorrect experts, but count them again...\n",
        "\n",
        "Here, we'd like to update the the experts' weights in a way such that correctness is rewarded and incorrectness is punished.\n",
        "\n",
        "From here, we will probabilistically make a decision. Experts with higher weight have more probability mass, and vice versa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-_ja9YOYOsU"
      },
      "source": [
        "# Ensemble Methods\n",
        "\n",
        "Now equipped with the multiplicative weights framework, let us explore the applications of this into machine learning, through ensemble methods. Here we will use sklearn to implement random forests and adaboost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx5RBfijYe81"
      },
      "source": [
        "# Step 1: Creating the data set\n",
        "\n",
        "Sklearn has nice data sets we can use for this. For example, let us explore the moons data set. Here we get the data and partition it into training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GStJLg6HZJpA"
      },
      "source": [
        "X, y = make_circles(n_samples=10000, noise=.5, random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6L1C1H-ZqCe"
      },
      "source": [
        "# Random Forests (Bagging)\n",
        "\n",
        "First, let us use a bagging method that was introduced to you previously, random forests. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yElPyhrgZmfI"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSsP71_wZUH7",
        "outputId": "7408d9d7-de06-4e2e-fa2f-f757684100cc"
      },
      "source": [
        "classifier = RandomForestClassifier(n_estimators=100, max_features=\"auto\",random_state=0)\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.529"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYpK0c33VF6P"
      },
      "source": [
        "## Boosting \n",
        "\n",
        "Now, let's think back to decision trees. we can apply this multiplicative weight update method to our decision trees, where the \"experts\" are our data points, and we upweight the misclassified points then run DTL lagorithm, repeatedly, combining the trees from each round\n",
        "\n",
        "Metacognitive whoa!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snP3vxKPVFM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0183423-f063-4580-8c98-07ea3efb4507"
      },
      "source": [
        "classifier = AdaBoostClassifier(n_estimators=100)\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5595"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-ZGq1EeWq1H"
      },
      "source": [
        "# **Conclusion**\n",
        "\n",
        "Thus, ensemble methods have their place as a fundamental range of techniques in machine learning. As the adage goes, the more the merrier. By intelligently combining the expertise of multiple models, we can surpass old limits. Now, you understand from a general sense, the intuition of ensemble methods by exploring the multiplicative weight updates algorithm and framework and how it applies to machine learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB6loV1Ya3gg"
      },
      "source": [
        "# Optional:\n",
        "\n",
        "Furthermore, adaboost may be generalized to gradient boosting. You may code that method up here and compare its performance to our previous methods!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKd7qWOPWqdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8eada94-2194-4791-c9b6-10c73be96271"
      },
      "source": [
        "classifier = GradientBoostingClassifier(n_estimators=100)\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    }
  ]
}