# Boosting
This assignment is going to take a relatively different approach to introducing boosting. Instead of introducing boosting within the scope of some machine learning application such as decision trees (which was assumedly covered in a prior week), we will first look more broadly at the general algorithmic framework of multiplicative weight updates (MWU), which is really just another flavor of something you've already seen in classes like EECS16AB through Lagrange Interpolation and Orthogonal Matching Pursuit (OMP). Then, we will revisit decision trees and random forests, and apply our MWU framework .

The idea of aggregating information from many sources has great utility in machine learning and is the basis behind ensemble methods. Boosting is one example of an ensemble method which has had great success in real-life applications. Indeed, the fundamental ideas behind these concepts are universal, and you have seen them in previous classes such as EECS16B through 

## Learning Objectives
Upon completion of this lesson, the student will be able to:

1. Identify and use the Multiplicative Weights Update (MWU) method as a general framework in a wide variety of problems. Identify the intimate connection between MWU and basic concepts in linear algebra, and look at problems that we've seen before in courses like EECS16AB, such as OMP, through the lens of MWU. 

2. Understand the basic ideas of Ensemble Methods in machine learning and the idea of combining several classifiers to create better ones.

3. Apply the AdaBoost Algorithm to Decision Trees to create strong classifiers from several weak ones, and view this as a particular application of the MWU framework. 

## How our assignment helps with that

Our assignment opens by allowing students to experiment with a few MWU strategies and get a feel for how they combine the advice of multiple experts to make the best possible decision. It then transitions over to exploring the idea of Ensemble Methods, with an example for Bagging as a warm-up. 

The students then have the challenge of implementing AdaBoost on Decision Trees. We have provided a rough framework for their code structure and enough starter code so that students have a general idea on how to go about the implementation; in addition, we have provided the pseudocode from the notes as a reference. Student can complete the implementation and then compare their results to scikit-learn's AdaBoost for comparison. In addition, we have several useful visuals for the classification boundaries generated by Boosting. Finally, there is an optional section for students interested in applying the technique to gradient boosting. 

## Navigating this repository

Note is contained in Multiplicative Weights and Boosting Note.pdf

Slide Deck is contained in Multiplicative Weights and Boosting Slide Deck.pdf

Code is contained in Multiplicative Weights and Boosting.ipynb

Quiz Questions are contained in Multiplicative Weights and Boosting Quiz Questions.pdf
