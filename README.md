# Boosting
This assignment is going to take a relatively different approach to introducing boosting. 
Instead of introducing boosting within the scope of some machine learning application such as decision trees (which was assumedly covered in a prior week), 
  we will first look more broadly at the general algorithmic framework of multiplicative weights, 
  which has wide applications in fields as diverse as game theory and finance. 
 Then, we will go back to boosting through decision trees and random forests.

The idea of aggregating information from many sources has great utility in machine learning and is the basis behind ensemble methods. 
Boosting is one example of an ensemble method which has had great success in real-life applications. 
Indeed, the fundamental ideas behind these concepts are universal, 
  and you have seen them in previous classes such as EECS16B through Lagrange Interpolation and Orthogonal Matching Pursuit (OMP). 
  
Navigating this repository

Note is contained in Multiplicative Weights and Boosting Note.pdf

Slide Deck is contained in Multiplicative Weights and Boosting Slide Deck.pdf

Code is contained in Multiplicative Weights and Boosting.ipynb

Quiz Questions are contained in Multiplicative Weights and Boosting Quiz Questions.pdf
